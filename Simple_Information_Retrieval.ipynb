{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNtazqpnZe2qspvHYDhaTkn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/reeveboy/Simple-Information-Retrieval/blob/main/Simple_Information_Retrieval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "imONEhA-zzTi",
        "outputId": "c03a0935-d286-415b-cb06-a1e278e83412"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# loading basic packages\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RjmjkEdQ0z9Z",
        "outputId": "d89aa835-471f-4034-b346-ee2f7e959554"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from nltk import word_tokenize\n",
        "import re"
      ],
      "metadata": {
        "id": "0187acwx0Why"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_xml(\"/content/drive/MyDrive/datasets/Mechanics of Search/cran.all.1400.xml\")\n",
        "df = df[~df['text'].isna()] # remove null data\n",
        "\n",
        "queries = pd.read_xml(\"/content/drive/MyDrive/datasets/Mechanics of Search/cran.qry.xml\")"
      ],
      "metadata": {
        "id": "ZLg0MRnu0Ehj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords as nltk_en_stopwords\n",
        "from spacy.lang.en import stop_words as spacy_en_stopwords\n",
        "NLTK_EN = set(nltk_en_stopwords.words('english'))\n",
        "SPACY_EN = spacy_en_stopwords.STOP_WORDS\n",
        "stop_words = NLTK_EN.union(SPACY_EN)"
      ],
      "metadata": {
        "id": "poGXclnB04pm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(doc_text):\n",
        "  text = doc_text.lower()\n",
        "  text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text) # Removes special characteres\n",
        "\n",
        "  tokens = word_tokenize(text) # tokenize text\n",
        "\n",
        "  text = [words for words in tokens if words not in stop_words] # remove stop words\n",
        "\n",
        "  ps = nltk.stem.PorterStemmer()\n",
        "  stemmed = [ps.stem(words) for words in text] # Stem words to the root form\n",
        "\n",
        "  return stemmed"
      ],
      "metadata": {
        "id": "F5igKNLi099r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import math\n",
        "from collections import Counter"
      ],
      "metadata": {
        "id": "FJdDgtIhq9Sc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vector Space Model"
      ],
      "metadata": {
        "id": "arC_cCCWlvmi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YIDK5-3Al2Vl",
        "outputId": "135a43d7-50e5-4525-dc9c-088732406179"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['docno', 'title', 'author', 'bib', 'text'], dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [' '.join([row['title'] or '', row['text'] or '']) for index, row in df.iterrows()]\n",
        "processed_corpus = [preprocess(doc) for doc in corpus]"
      ],
      "metadata": {
        "id": "2ykUd88klhw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VSM:\n",
        "  def __init__(self):\n",
        "    self.corpus = [' '.join(doc) for doc in processed_corpus]\n",
        "    self.vectorizer = TfidfVectorizer()\n",
        "    self.documents_vector = self.vectorizer.fit_transform(self.corpus)\n",
        "\n",
        "  def rank(self, query, top_n=100):\n",
        "    q = ' '.join(preprocess(query))\n",
        "    query_vector = self.vectorizer.transform([q])\n",
        "\n",
        "    cosineSimilarities = cosine_similarity(self.documents_vector, query_vector).flatten() # Calculate cosine similarities\n",
        "    related_docs_indices = cosineSimilarities.argsort()[::-1][:top_n]  # Get top_n indices\n",
        "\n",
        "    ranked_results = []\n",
        "    for rank, i in enumerate(related_docs_indices, start=1):\n",
        "        doc_id = df.iloc[i]['docno']\n",
        "        similarity_score = cosineSimilarities[i]\n",
        "        ranked_results.append((doc_id, similarity_score))\n",
        "\n",
        "    return ranked_results\n",
        "\n",
        "vsm_model = VSM()"
      ],
      "metadata": {
        "id": "gmJ5pGJXbf-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BM25 Model\n",
        "\n"
      ],
      "metadata": {
        "id": "d7BTUJIjldZB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BM25:\n",
        "  def __init__(self, k1=1.2, b=0.75):\n",
        "    self.corpus = processed_corpus\n",
        "    self.k1 = k1\n",
        "    self.b = b\n",
        "    self.avgdl = sum(len(doc) for doc in self.corpus) / len(self.corpus)\n",
        "    self.idf = self.calculate_idf()\n",
        "\n",
        "  def calculate_idf(self):\n",
        "    idf = {}\n",
        "    doc_count = len(self.corpus)\n",
        "    for doc in self.corpus:\n",
        "      word_set = set(doc)\n",
        "      for word in word_set:\n",
        "        idf[word] = idf.get(word, 0) + 1\n",
        "\n",
        "    for word, count in idf.items():\n",
        "      idf[word] = math.log((doc_count - count + 0.5) / (count + 0.5) + 1)\n",
        "\n",
        "    return idf\n",
        "\n",
        "  def get_bm25_score(self, query: list[str], doc: list[str]) -> float:\n",
        "    score = 0\n",
        "    doc_counter = Counter(doc)\n",
        "    doc_len = len(doc)\n",
        "    for word in query:\n",
        "      if word not in doc_counter:\n",
        "        continue\n",
        "      idf = self.idf.get(word, 0)\n",
        "      f = doc_counter[word]\n",
        "      score += idf * ((f * (self.k1 + 1)) / (f + self.k1 * (1 - self.b + self.b * (doc_len / self.avgdl))))\n",
        "    return score\n",
        "\n",
        "  def rank(self, query: str, top_n: int = 100) -> list[tuple[str, float]]:\n",
        "    q = preprocess(query)  # You should define preprocess function\n",
        "    scores = []\n",
        "\n",
        "    for i, doc in enumerate(self.corpus):\n",
        "      score = self.get_bm25_score(q, doc)\n",
        "      scores.append((i, score))\n",
        "\n",
        "    scores.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    ranked_results = []\n",
        "    for rank, (idx, score) in enumerate(scores[:top_n], start=1):\n",
        "      doc_id = df.iloc[idx]['docno']  # Assuming df is defined somewhere\n",
        "      similarity_score = score\n",
        "      ranked_results.append((doc_id, similarity_score))\n",
        "\n",
        "    return ranked_results\n",
        "\n",
        "bm25_model = BM25()"
      ],
      "metadata": {
        "id": "zH9VpSXqzX9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Relevance Model"
      ],
      "metadata": {
        "id": "5BN-Ouhxl3yc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RelevanceModel:\n",
        "  def __init__(self, mu=1000, lambda_=0.6, top_n=10, top_terms=15):\n",
        "    self.corpus = [' '.join(doc) for doc in processed_corpus]\n",
        "    self.mu = mu\n",
        "    self.lambda_ = lambda_\n",
        "    self.top_n = top_n\n",
        "    self.top_terms = top_terms\n",
        "    self.vectorizer = TfidfVectorizer()\n",
        "    self.documents_tfidf = self.vectorizer.fit_transform(self.corpus)\n",
        "    self.vocab = np.array(self.vectorizer.get_feature_names_out())\n",
        "\n",
        "  def rank(self, query, n=100):\n",
        "    query = ' '.join(preprocess(query))\n",
        "\n",
        "    # Initial ranking\n",
        "    initial_scores = self.rank_documents(query)\n",
        "    top_docs = [idx for idx, _ in initial_scores[:self.top_n]]  # Select top documents\n",
        "\n",
        "    # Feedback query expansion using top documents\n",
        "    relevant_docs_tfidf = self.documents_tfidf[top_docs]\n",
        "    new_query = self.expand_query(query, relevant_docs_tfidf)\n",
        "\n",
        "    # Re-rank documents using the new query\n",
        "    re_ranked_scores = self.rank_documents(new_query)\n",
        "\n",
        "    ranked_results = []\n",
        "    for idx, score in re_ranked_scores[:n]:\n",
        "      doc_id = df.iloc[idx]['docno']\n",
        "      similarity_score = score\n",
        "      ranked_results.append((doc_id, similarity_score))\n",
        "\n",
        "    return ranked_results\n",
        "\n",
        "  def rank_documents(self, query):\n",
        "    query_tfidf = self.vectorizer.transform([query])\n",
        "    similarities = cosine_similarity(query_tfidf, self.documents_tfidf)\n",
        "    similarities = similarities.flatten()\n",
        "    document_scores = [(i, score) for i, score in enumerate(similarities)]\n",
        "    document_scores.sort(key=lambda x: x[1], reverse=True)\n",
        "    return document_scores\n",
        "\n",
        "  def expand_query(self, original_query, relevant_docs_tfidf):\n",
        "    # Calculate query likelihood P(t | R) for each term\n",
        "    query_tfidf = self.vectorizer.transform([original_query])\n",
        "    doc_length = relevant_docs_tfidf.sum(axis=1)\n",
        "    doc_length_norm = doc_length / (doc_length + self.mu)\n",
        "    query_likelihood = np.asarray(relevant_docs_tfidf.sum(axis=0) / doc_length_norm).flatten()\n",
        "\n",
        "    # Get top terms with higher P(t | R)\n",
        "    top_term_indices = np.argsort(query_likelihood)[::-1][:self.top_terms]\n",
        "    top_term_indices = top_term_indices[top_term_indices < len(self.vocab)]\n",
        "    top_terms = self.vocab[top_term_indices]\n",
        "\n",
        "    # Interpolate with original query\n",
        "    original_query_tfidf = self.vectorizer.transform([original_query])\n",
        "    new_query_tfidf = self.lambda_ * query_likelihood[top_term_indices] + (1 - self.lambda_) * original_query_tfidf[:, top_term_indices]\n",
        "\n",
        "    # Construct new query\n",
        "    new_query = \" \".join(top_terms)\n",
        "    return new_query\n",
        "\n",
        "relevance_model = RelevanceModel()"
      ],
      "metadata": {
        "id": "BcNVKS-fl5ye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rank-bm25"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3a9ukC6_uO8",
        "outputId": "439c5efd-296f-4b67-dda0-a55cd12f050e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rank-bm25 in /usr/local/lib/python3.10/dist-packages (0.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rank-bm25) (1.25.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def output_results_to_trec_format(results, model_name, output_file):\n",
        "    with open(output_file, 'w') as f:\n",
        "        for query_id, docs in results.items():\n",
        "            for rank, (doc_id, score) in enumerate(docs, start=1):\n",
        "                line = f\"{query_id} 0 {doc_id} {rank} {score} {model_name}\\n\"\n",
        "                f.write(line)"
      ],
      "metadata": {
        "id": "-Xq2pNjykn-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dictionary to store results for each query\n",
        "vsm_results = {}\n",
        "bm25_results = {}\n",
        "relevance_results = {}\n",
        "\n",
        "# Execute VSM model for each query\n",
        "for query_info in queries.to_dict(orient='records'):\n",
        "  query_num = query_info['num']\n",
        "  query_title = query_info[\"title\"]\n",
        "  vsm_result = vsm_model.rank(query_title, top_n=100)  # Execute VSM model for the query\n",
        "  vsm_results[query_num] = vsm_result\n",
        "\n",
        "  bm25_result = bm25_model.rank(query_title, top_n=100)  # Execute BM25 model for the query\n",
        "  bm25_results[query_num] = bm25_result\n",
        "\n",
        "  relevance_result = relevance_model.rank(query_title, n=100)  # Execute RelevanceModel for the query\n",
        "  relevance_results[query_num] = relevance_result\n",
        "\n",
        "!rm vsm_results.txt\n",
        "!rm bm25_results.txt\n",
        "!rm relevance_results.txt\n",
        "\n",
        "# Output VSM results to file in TREC format\n",
        "output_results_to_trec_format(vsm_results, \"vsm\", \"vsm_results.txt\")\n",
        "output_results_to_trec_format(bm25_results, \"bm25\", \"bm25_results.txt\")\n",
        "output_results_to_trec_format(relevance_results, \"relevance\", \"relevance_results.txt\")"
      ],
      "metadata": {
        "id": "Vf2uddkKtsl5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/usnistgov/trec_eval.git\n",
        "!make -C trec_eval"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6vIY7uPZqFip",
        "outputId": "4658a958-c622-407c-9891-20a6a6b73bb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'trec_eval' already exists and is not an empty directory.\n",
            "make: Entering directory '/content/trec_eval'\n",
            "make: 'trec_eval' is up to date.\n",
            "make: Leaving directory '/content/trec_eval'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download cranqrel.trec.txt file\n",
        "!wget https://raw.githubusercontent.com/oussbenk/cranfield-trec-dataset/main/cranqrel.trec.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDycJXP5xBTT",
        "outputId": "17c78e72-e654-4404-dac2-6c18e907dd45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-03-15 15:20:00--  https://raw.githubusercontent.com/oussbenk/cranfield-trec-dataset/main/cranqrel.trec.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 23217 (23K) [text/plain]\n",
            "Saving to: ‘cranqrel.trec.txt.2’\n",
            "\n",
            "cranqrel.trec.txt.2 100%[===================>]  22.67K  --.-KB/s    in 0.002s  \n",
            "\n",
            "2024-03-15 15:20:01 (12.9 MB/s) - ‘cranqrel.trec.txt.2’ saved [23217/23217]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to your cranqrel.trec.txt and vsm_results.txt files\n",
        "path_to_cranfield_qrel = \"/content/cranqrel.trec.txt\"\n",
        "path_to_vsm_results = \"/content/vsm_results.txt\"\n",
        "path_to_bm25_results = \"/content/bm25_results.txt\"\n",
        "path_to_relevance_results = \"/content/relevance_results.txt\"\n",
        "\n",
        "# Run trec_eval with specific evaluation measures\n",
        "!./trec_eval/trec_eval -m P.5 -m ndcg -m map {path_to_cranfield_qrel} {path_to_vsm_results}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ZN4_q-iv9Ez",
        "outputId": "6668e659-492e-42f0-eab8-712469876eec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "map                   \tall\t0.0105\n",
            "P_5                   \tall\t0.0158\n",
            "ndcg                  \tall\t0.0408\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./trec_eval/trec_eval -m P.5 -m ndcg -m map {path_to_cranfield_qrel} {path_to_bm25_results}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9XVy7GKR8qKe",
        "outputId": "eb7afcaf-e2a2-464c-fca8-c817c9ee58ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "map                   \tall\t0.0086\n",
            "P_5                   \tall\t0.0132\n",
            "ndcg                  \tall\t0.0365\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./trec_eval/trec_eval -m P.5 -m ndcg -m map {path_to_cranfield_qrel} {path_to_relevance_results}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1kD7MRR826m",
        "outputId": "d0b6781d-c598-4c9f-bf35-5995d6b2d0ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "map                   \tall\t0.0116\n",
            "P_5                   \tall\t0.0145\n",
            "ndcg                  \tall\t0.0420\n"
          ]
        }
      ]
    }
  ]
}